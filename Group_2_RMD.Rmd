---
title: "Churn_project"
author: "Vijay"
date: "12/6/2019"
output: html_document
---
```{r}
library(C50)
library(caret)
library(gmodels)
library(ggplot2)
library(corrplot)
library(dplyr)
library(pROC)
data(churn)

# I Model
# Constructing the models with all the variables except State varialbe.
data.train<-churnTrain[,-1]
data.train$churn<-factor(data.train$churn,levels(data.train$churn)[c(2,1)])
data.test<-churnTest[,-1]
data.test$churn<-factor(data.test$churn,levels(data.test$churn)[c(2,1)])

# Applying Logistic regression on the training data.
model<- glm(churn~.,family = binomial(link='logit'),data = data.train)

# Predicting the churn on test data.
result<-predict(model,data.test,type = 'response')
# setting the cutoff value of the probalilty of prediction to greater that 60%
result_labels<-as.factor(ifelse(result>0.6,'yes','no'))

# Checking the accuracy of the prediction.
CrossTable(data.test$churn,result_labels,prop.chisq = FALSE)
confusionMatrix(result_labels,data.test$churn)$overall[1]

# Performance metric of the model is given by AUC.
roc(data.test$churn,result)
# Area under the curve: 0.8392

# II Model
# Let us now reconstruct the model by reducing the variables and check the accuracy of the prediction.  
# Finding the higly correlated variables using corrplot by excluding the categorical variables.
corrplot(cor(churnTrain[,-c(1,3,4,5,20)]))
# Eliminating the one of the highly correlated variables.
train<-churnTrain[,-c(1,7,10,13,16)]
train$churn<-factor(train$churn,levels(train$churn)[c(2,1)])
test<-churnTest[,-c(1,7,10,13,16)]
test$churn<-factor(test$churn,levels(test$churn)[c(2,1)])

#Logistic regression model
model1<- glm(churn~.,family = binomial(link='logit'),data = train)
summary(model1)

#Predicting the labels
result1<-predict(model1,newdata = test,type = 'response')
result1_labels<-as.factor(ifelse(result1>0.6,'yes','no'))

#Accuracy of the model
CrossTable(test$churn,result1_labels,prop.chisq = FALSE)
confusionMatrix(result1_labels,test$churn)$overall[1]

#Performance metric of the model
roc(test$churn,result1)
# Area under the curve: 0.84


# III Model
# From the above model, I'll choose the statistically significant varialbes to simplify my model futher.
summary(model1)
# From the summary of II model only choose the variables whose probability of not rejecting null hypothesis is very low i.e, Pr(>|z|)<0.05
train1<-churnTrain[,c(4,5,6,9,12,15,18,19,20)]
train1$churn<-factor(train1$churn,levels(train1$churn)[c(2,1)])
test1<-churnTest[,c(4,5,6,9,12,15,18,19,20)]
test1$churn<-factor(test1$churn,levels(test1$churn)[c(2,1)])

#Logistic regression model
model2<- glm(churn~.,family = binomial(link='logit'),data = train1)
summary(model2)

#Predicting the labels
result2<-predict(model2,newdata = test1,type = 'response')
result2_labels<-as.factor(ifelse(result2>0.6,'yes','no'))

#Accuracy of the model
CrossTable(test1$churn,result2_labels,prop.chisq = FALSE)
confusionMatrix(result2_labels,test1$churn)$overall[1]

#Performance metric of the model
roc(test$churn,result2)
# Area under the curve: 0.8445
plot.roc(test$churn,result2)

# From the above observations Model III has only 9 out of 20 varialbes with better AUC of 84.45% than other models. Hence it is desirable to choose the simplex model with best AUC.

##### Now to predict the churn of the new data.
load("C:/Users/Vijay/Downloads/Customers_To_Predict (2).RData")
new_result<-predict(model2,newdata = Customers_To_Predict,type = 'response')
new_result_labels<-as.factor(ifelse((new_result>0.6),'yes','no'))


# Visualizing the statewise customers who are going to churn or not.
Customers_To_Predict$labels<-new_result_labels
cust_data<-Customers_To_Predict %>% group_by(state)
# Plot
ggplot(cust_data) +
  aes(x = state,fill = labels) +
  geom_bar()
# From the above plot, we can target the customers of statewise who are going to churn so that management can try to retain them with appropriate plans. 
# The major reason of customer churn is service levels of the service_provider.
ggplot(cust_data) +
  aes(x = factor(number_customer_service_calls),fill = labels) +
  geom_bar()+
  scale_fill_hue() + 
  theme_minimal()+
  facet_wrap(vars(c(area_code)))
# From the above graph we can infer that the more number of service calls made the more likely a customer would churn 

```

